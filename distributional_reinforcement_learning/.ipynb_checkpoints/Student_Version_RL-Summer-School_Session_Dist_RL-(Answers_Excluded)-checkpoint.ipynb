{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributional Reinforcement Learning: Practical Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 5214,
     "status": "error",
     "timestamp": 1655991959184,
     "user": {
      "displayName": "Jesse van Remmerden",
      "userId": "16560922049388817820"
     },
     "user_tz": -120
    },
    "id": "PxKeKXB5z54P",
    "outputId": "671247fa-f72b-4a8c-d14f-c3dbd9d60b82",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import necessary libraries \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from random import shuffle\n",
    "from collections import deque\n",
    "from variables import *\n",
    "\n",
    "from IPython.display import YouTubeVideo\n",
    "from dist_dqn import dist_dqn\n",
    "from auxiliary import preproc_state, get_dist_plot\n",
    "from trainer import trainer, lossfn, get_action_dqn, get_target_dist\n",
    "\n",
    "import imageio\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NCKeR_4z54U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Distributional Reinforcement Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQJNo1s7z54V",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Expectation Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLnGaoBHz54V",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will very briefly discuss how to compute the expectation. Recall that the expectation is essentially the weighted average of each of the values in the sample space. Formally\n",
    "\n",
    "$$\\large \\mathbb{E}[X] = \\sum_{i=1}^{n}x_{i}p_{i},$$\n",
    "\n",
    "where $\\mathbb{E}[X]$ denotes the expected value of a random variable $X$, $x_{i}$ a given outcome and $p_{i}$ the probability of that outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URPifP0rz54W",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In python, we can compute it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nREmxvF9z54W",
    "outputId": "e306d815-216f-43a7-f4f4-3205119e9337",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "probs = np.array([0.6, 0.1, 0.1, 0.1, 0.1])\n",
    "outcomes = np.array([18, 21, 17, 17, 21])\n",
    "expected_value = 0.0\n",
    "\n",
    "for i in range(probs.shape[0]):\n",
    "    expected_value += probs[i] * outcomes[i]\n",
    "\n",
    "print(expected_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kEKvSwEz54W",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more succinct way, however, is to compute the expected value as the dot product of the possible outcomes and their corresponding probabilities. In Python, we can use the ``@`` operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKNEgj6qz54X",
    "outputId": "eba4bfc5-68b0-468e-f44f-32aa3aaf6cca",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "expected_value = probs @ outcomes\n",
    "print(expected_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoKRFIX-z54Y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Bellman Equation Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tclZl9Wkz54Y",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Most reinforcement learning algorithms determine the value of an action using the *Bellman* equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\large Q (x,a) = \\mathbb{E}[R(x,a)] + \\gamma \\mathbb{E}[Q(X', A')],$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "where $Q (x,a)$ denotes the q-function of a state-action pair, $\\gamma$ the discount factor and $Q(X', A')$ the value of the state and action at the next time step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRg5RtLKz54Y",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The *Distributional* Bellman Equation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNbY3Q7Iz54Y",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A key difference between \"regular\" reinforcement learning and distributional reinforcement learning (DRL) is that in DRL, we do not \"collapse\" the value estimate into one single number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In distributional RL, we are interested in learning the *entire* **value distribution**, which we can formalise it as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\large Z(x, a) = R(x,a) + \\gamma Z(X', A'),$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "where $Z$ takes the place of $Q$. Note that $X' \\sim  P(\\cdot | (x, a))$ and $A' \\sim  \\pi (\\cdot| X')$ are random variables and $\\gamma$ is the discount term. \n",
    "\n",
    "We use $Z$ to make clear that we are talking about a *distribution* rather than a singular Q-value. Important to understand, moreover, is that by using $Z$ we seek to express the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Randomness in the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Randomness in the transition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Randomness in the next-state value distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As such, in distributional reinforcement learning we do not estimate the value function, but seek to estimate the value distribution for a state-action pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Categorical Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWfXTEYcz54Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Context, Scenario and Gameplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBnz2CsDz54X",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In distributional RL, instead of talking about the sample space (which can be infinite), we make use of a **support** set. The support is essentially the same as the sample space, but bounded on both sides. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPy8bOL9z54Z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "An essential part of distributional reinforcement learning entails *updating* a *projection* of the distributional Bellman equation. In this section, we will discuss and implement such an updating procedure. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the scenario where we start with a situation in which we have a *sample transition* $(x, a, r, x')$, where $x$ denotes the current state, $a$ the action that is taken, $r$ the reward of that action and $x'$ the next state the agent would go to by taking that action.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4SsHBaez54Z",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make things a bit more concrete, we will implement an updating procedure with a specific setting in mind, the game of Freeway. Freeway is an Atari game where the goal is for a chicken to cross  a highway without being hit by a car. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZRwtRc3z54a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The reward structure of this game is as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We received +10 reward if the chicken successfully crosses the highway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the game is lost (the chicken does not manage to cross the street within a certain amount of time), we get a reward of -10. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- At all non-terminal steps, we get a reward of -1. We do this to make sure that the agent *acts* and does not wait for too long. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K6dst1Az54a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moreover, we have 3 possible actions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- NOOP. No operation, do nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- DOWN. Go down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- UP. Go up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OHJhuJgz54a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By means of a short illustration, consider the following clip:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Y23Urr_z54a",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"img/freeway.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Quick sidenote:** Note that, at all times, we assume a that the agent follows a greedy policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Quick sidenote:** Also important to note here is that we will essentially use a Bayesian updating procedure, meaning that we update a prior distribution based on information we encounter from the environment (i.e. the rewards we get)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wZ2EKWYz54b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKdZsqJzz54b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We update the distribution in accordance with a simple updating rule. We start with a prior and gradually update it to a ***posterior*** (target) distribution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here we will follow the approach proposed by [Bellemare et.al. (2017)](https://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf) for **approximate distributional reinforcement learning**. We will implement a slightly *modified* version of their **Categorical Algorithm**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Categorical Algorithm (displayed below) takes a sample transition as its input ($x_{t}$, $a_{t}$, $r_{t}$, $x_{t+1}$), a discount factor ($\\gamma_{t}$), a q-value function ($Q_{x_{t+1}. a}$), an optimal successor state ($a^{*}$) and an index tracker ($m_{i}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/algorithm.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The algorithm consists of two parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. The first part computes the projection onto the support vector. This simply means that we locate, on the support vector, where the centre of mass lies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. The second part, which determines how much probability will be redistributed from the other points on the support to $\\hat{T}z_{j}$ and to neighboring points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick sidenote:** Note that we will not implement the cross-entropy loss in this notebook. If you are interested in its implementation, please have a look at the function ``lossfn`` in the ``trainer.py``-file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Support and Probability Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cw9nGcYqz54b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First of all, it is important to cut off the reward distribution. If we would not do this, we would end up with an infinite range. As such, our distribution will be parameterized by two values, $V_{\\text{MIN}}$ and $V_{\\text{MAX}}$ (and *N*, discussed later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSt-k9zEz54b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since the most negative reward we can receive in Freeway is -10 and the most positive reward is +10, we will fix these two values as the limits of the reward distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epW4NC6Cz54b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vmin,vmax = -10.,10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2wCMUe8z54b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Another key concept discussed before is the **support**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The support is the set of all possible outcomes we can obtain, bounded by $V_{\\text{min}}$ and $V_{\\text{max}}$, *and* binned. In our case, this can be -10, +10, but also -1. Because of the probabilistic nature of the environment, however, we may also obtain reward values of, say, -4.2 or 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_osFlgBWz54c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For computational purposes, we work with *discrete* distributions. As a result, all reward values need to be *binned* before we can actually work with them. Here we select a prior distribution of 51 possible outcomes. These 51 values are the **support values** (or atoms) of our distribution and the length of the support will be denoted by *N*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the literature, this is called a 51-atom agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiRBAC0wz54c",
    "outputId": "aea442ef-bb09-47f9-dae5-a37b5f4bce37",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "N=51  # number of items in the support \n",
    "support = np.linspace(vmin,vmax,N) \n",
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04HK7cNTz54c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Quick sidenote:** We could have chosen a different support number. Here we selected 51, because the difference between the bins is 0.4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA6YYo0yz54c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us now construct the actual prior distribution. Note that because we do not know anything about the reward structure of the environment, we assume that each outcome is equally likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As such, we initialize our prior distribution as a uniform distribution over 51 possible outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0noQ0q4Dz54c",
    "outputId": "32af3774-419f-40e1-8707-f13214c30b12",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = np.ones(N)\n",
    "prior = probabilities / probabilities.sum()\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-KFTCV0z54d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can visualize our distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_HHJIfGz54d",
    "outputId": "2e80e851-578b-45c2-fd0a-7ed777d7c0ad",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6,5)\n",
    "plt.bar(support,prior, color = 'green')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Prior Reward Distribution')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siDngEWnz54d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that we have two vectors: the ``support`` vector (which corresponds to all events with non-zero probability) and the ``prior`` vector which holds the probabilities for the values in the support vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7JMF6J_z54d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Locating the Reward Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKYbcVc4z54d",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because we want to construct a projection of the reward onto the support vector, we need to compute the index of the reward on that support vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, our goal here will be to write some code that does this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "endxjV2_z54d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us first fix some parameters and introduce a variable Z. Here we assume a $\\gamma$ of 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3w3uPiK3z54e",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.8\n",
    "Z = prior    # rename prior to Z\n",
    "N=51         # number of items/atoms in the support "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the context of the categorical algorithm (see: algorithm above), what we want to do now is to compute the **Bellman Update** by gradually updating the components of the update (i.e. each atom $z_{j}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose now that we are in a given state $x$ and that the $a_{\\text{up}}$ is the action that yields the highest reward (of say, $5$). We need to find a way to project this value onto the support distribution. As such, the first step would be to *locate the index* of the reward value on the support vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, first want to to locate the location of the reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmhKw7K0z54e",
    "outputId": "6fe8379f-f9aa-455c-e82f-808396192fc3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Assume that for a given action, we get a reward of 5\n",
    "observed_reward = 5\n",
    "plt.bar(support,Z, color = 'green')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Prior Reward Distribution')\n",
    "plt.axvline(x = observed_reward, color = 'r', label = 'axvline - full height')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJF4HnH5z54f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Moreover, we observe that the spacing between the support values is 0.4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61N2Kn36z54e",
    "outputId": "81503e3b-7304-43f8-eb47-37837501beab",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, we do not have to need to rely on observation, but can also derive it using the following formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\large \\Delta z = \\frac{V_{MAX} - V_{MIN}}{N-1}, $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "where $\\Delta z $ denotes the closeness between the elements in the support vector and $N$ the number of elements in the support vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TG_iS2WCz54f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 1**:  Verify that the space between each element is indeed 0.4 by implementing the equation above. Make sure that the variable name is ``dz``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHP1tmoIz54f",
    "outputId": "7662db85-6a3c-4398-f4c1-9164ec05b637",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j5nW3CAz54f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a next step, we want to pinpoint exactly where the value $5$ falls on the support vector. It is highly likely that the observed reward will not fall exactly on one of the elements in the support vector. To reiterate, we need to find the *index* value of the ***closest* support element**, which we will call $b_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can use the following formula to find the closest support element:\n",
    "\n",
    "$$\\large b_{j} = \\frac{r-V_{MIN}}{\\Delta z},$$\n",
    "\n",
    "where $r$ denotes the observed reward. \n",
    "\n",
    "**Quick sidenote:** For the sake of simplicity, contrary to what is stated in the algorithm, we replaced $\\hat{T}z_{j}$ by $r$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuMRb6CPz54f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Quick sidenote:** Note that $b_{j}$ can take on any value in the support vector. In this case, $b_{j} \\in [0, N - 1].$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjX4K4mJz54g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 2:** Implement the equation above. Make sure that $b_{i} \\in \\mathbb{Z}$ (i.e. make sure that $b_{i}$ has an integer value). Also make sure to name the variable ``bj``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ap6y7r2z54g",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# compute bj, the index value of the support vector \n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuroYo_2z54g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Evaluate the value of ``bj``. Does it correspond to the location of the reward ``5`` in the plots above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gzdgz5Wmz54g",
    "outputId": "a0f93cc4-1268-49ca-cf56-873334e9014b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NF4rGkfOz54g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prior to Posterior: Updating the Probability Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxWl5Ptqz54g",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because it is convenient to keep a copy of the original distribution, we create a copy of the original ``probabilities`` vector and call it ``posterior``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uts5tBLkz54h",
    "outputId": "9c339fbb-000c-48f6-85e4-0aa81d01efad",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "posterior = prior.copy()\n",
    "print(posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGr6f0Idz54h",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Every time the agent encounters a reward, we update the posterior distribution of the action that yields the highest reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One way to think of this is that probability mass is shifted from the left and right sides of the reward value to the value of the observed reward. In other words, probability mass gets *redistributed* towards the centre of mass.  This is the second step in the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-EbIV8ez54h",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As illustrated below, our goal here is to update the uniform distribution by gradually moving mass to the centre of mass, both from the left side and from the right side. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRg2hTy-z54h",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![alt text](img/bayes.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To put all of this in more Bayesian terms, our belief is that $5$ is representative of the actual reward structure in the environment is being *reinforced*. This means that all other values are *less likely* and that observing a $5$ is *more likely*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02dYkwBUz54h",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By now it should become clear why computing the index of the support vector was so important: we need it to determine *where*, on the probabilities vector, we need to move the mass to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_l4jVdHz54i",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As such, in a way, the ``support`` acts as our sample space and the ``probabilities`` (or ``Z``) vector keeps track the probabilities of the support elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzXQTw1Wz54i",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Thus, in the context of approximate distributional reinforcement learning, **updating the target distributions** refers to the agent making an update to the empirical reward distribution each time it receives a reward. In doing so, it strengthens its belief for a certain action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJoeVFOCz54i",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, for each point in the probabilities vector (remember, there are 51 items in that vector), we gradually want to update the values by stealing some mass from its immediate neighbor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iupIeG6z54i",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make things a bit more precise, we will follow a two-step procedure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. We will steal mass from the neighbors left of $b_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. We will steal mass from the neighbors right of $b_{j}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8AubN5uz54i",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us consider a couple of cases of \"mass-stealing\" for the neighbors **left** of $b_{j}$, which we can formalize as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lrVjtNtz54i",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ \\large m^{i}_{l} \\leftarrow m^{i}_{l} + \\gamma^{j} m^{i-1}_{l},$$\n",
    "\n",
    "where $m_{l}^{i}$ denotes the probability mass of the value we want to update, $\\gamma^{j} m^{i-1}_{l}$ how much we steal from its neighbor on the left and $\\gamma^{j}$ denotes the discount factor with an index $j$ (this tells us how far we are removed from the centre of mass - the further away from $b_{j}$ the fewer mass is being stolen).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cK8bQs7Az54j",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Important to note here is that *each* value of the distributions is being updated. As can be seen in the formula, the update always contain the value of $m^{i}_{l}$ itself plus a discounted update term, $\\gamma^{j} m^{i-1}_{l}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBlwA28xz54i",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 3:** Consider the following loop and observe how the value changes for different values of ``gamma``. How does the output changes for ``gamma = 1``, ``gamma = 0.8`` and ``gamma = 0.4``? And what happens as ``j`` increases? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjStY55Wz54j",
    "outputId": "267c6554-238c-4a14-9d22-044275461ab5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.4\n",
    "\n",
    "for j in range(1, 5):\n",
    "    print(f'j: {j}. Value for second term: {np.round(np.power(gamma, j), 2)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWicsiqjz54j",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The values around the observed reward value $r$ are more likely than values further down the line (e.g. if we observe 5, then 4 is more likely to be observed than -10). As such, we want to update each of the values in the probabilities (i.e. posterior) vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrIJKJv7z54j",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us look at a couple of examples. First, let us look at what an update of the value for $r=5$, which, as we computed, is located at $b_{j} = 38$, would look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# print prior probability for reward observation\n",
    "print(posterior[38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyBbIzzRz54j",
    "outputId": "888b83b1-1ac5-45a7-fc3a-cd28f8f7b35b",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "gamma = 0.8\n",
    "j = 1       # j indicates how far we are removed from the centre of mass bj\n",
    "updated_val = posterior[38] + ((gamma**j) * posterior[38-1])\n",
    "print(updated_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlEo2cvCz54j",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As can be seen, the new *probability value* at index *38* is now ~0.035 instead of ~0.020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TToZSfVXz54k",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us consider the next value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzDsxXySz54k",
    "outputId": "40d752b1-e357-4df0-d5aa-20cdc7ba5fc4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "j = 2       # j indicates how far we are removed from the centre of mass bj\n",
    "updated_val = posterior[37] + ((gamma**j) * posterior[37-1])\n",
    "print(updated_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTgTAdsyz54k",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the new value is ~0.032. As can be seen, the index parameter *j* reduces the amount of mass shifted from one point to the next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HivxkCgbz54k",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 4:** Now it is your turn. Write a for loop that gradually updates each of the values on the *left* side of the centre of mass at $b_{j}$. Make sure to update the values in the ``posterior`` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJwEGwGIz54k",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# redistribute probability mass for each of the values on the left side to the observed reward value\n",
    "# YOUR CODE HERE\n",
    "posterior = prior.copy()\n",
    "j = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsHxK6arz54k",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Verify the output. You should observe a step-pattern on the left side of $b_{j} = 38$. As you can imagine, we also want to update the points on the right of the centre of mass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwVxzuYnz54l",
    "outputId": "8128e323-8fc7-4d66-8cb5-fd86d52fb751",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(support,posterior, color = 'green')\n",
    "plt.title('Reward Distribution for an Action A')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lafl7JLcz54l",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 5:** Update the points on the *right* side using the following update rule:\n",
    "\n",
    "$$ \\large m^{i}_{r} \\leftarrow m^{i}_{r} + \\gamma^{j} m^{r+1}_{r},$$\n",
    "\n",
    "where $m_{r}^{i}$ denotes any point on the right of $b_{j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GPPx-Htz54l",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# redistribute probability mass for each of the values on the left side to the observed reward value\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fnpyf-YVz54l",
    "outputId": "2c46de07-fc2c-4959-8074-aa23ea91cc22",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(support,posterior, color = 'green')\n",
    "plt.title('Reward Distribution for an Action A')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2vLYjlGz54l",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Because we are dealing with a probability distribution, check if the sum still equals 1. If this is not the case, normalize the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OWmwwinz54m",
    "outputId": "90f457c4-a5dc-42a9-ba4f-044dbb495ab3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQSaAu86z54m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the code below. You should see a mode around the reward value of $5$, with a step pattern on both sides of this value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8Io4qtNz54m",
    "outputId": "c49f2c8f-4ab6-4faf-e9a7-4ae5afc8bf34",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.bar(support,posterior, color = 'green')\n",
    "plt.title('Updated Normalised Value Distribution for an Acion A')\n",
    "plt.ylabel('Probability')\n",
    "plt.axvline(x = observed_reward, color = 'r', label = 'axvline - full height')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoR_XOuRz54m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have now created all the necessary building blocks to build an updater function, which we will call ``update_dist``. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 6:** Combine all the individual pieces of code into a function that returns the updated posterior. Please do not change the input parameters. Moreover, make sure that the function returns the *entire* updated distribution. \n",
    "\n",
    "- Hint: You can reuse *all* your code from before - no need to write any new code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfzzYdy2z54m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def update_dist(observed_reward, support, probabilities, vmin, vmax, gamma=0.8):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDhzmEdNz54n",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run the code below and evaluate the output. Does it cluster around the observed reward value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vjMOri8z54n",
    "outputId": "14edbbd2-dce9-4df4-a4c9-c85bf1acbca1",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "observed_reward = 5    # set hypothetical reward\n",
    "number_of_obs = 10     # set the number of observations\n",
    "Z = prior              # initialise prior\n",
    "\n",
    "for i in range(number_of_obs):\n",
    "    Z = update_dist(observed_reward,support,Z,vmin,vmax,gamma=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(support,Z, color = 'green')\n",
    "plt.title('Reward Distribution for an Action A')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZE5mKobz54n",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the more often a certain reward is received by the agent, the higher the tendency to cluster around one value (or values, as we will see in a bit). As such, the variance decreases with more observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jeh_YZCz54n",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider the code below. We now have a more complicated reward structure. Does it cluster around the various modes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l20F7LX9z54n",
    "outputId": "df58d4b6-1850-4ecd-9d0e-d72954a968df",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z = prior\n",
    "ob_rewards = [10,10,10,0,1,0,-10,-10,10,10]\n",
    "for i in range(len(ob_rewards)):\n",
    "    Z = update_dist(ob_rewards[i], support, Z, vmin,vmax, gamma=0.7)\n",
    "\n",
    "plt.figure(figsize=(9,7))\n",
    "plt.bar(support, Z, color = 'purple')\n",
    "plt.title('Distribution with Complex Reward Structure for an Action A')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Rewards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spnD0Fxoz54n",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As can be seen, the reward structure is more complicated in this scenario. This can be hugely beneficial in the context of reinforcement learning because the agent can now make a more nuanced decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuHa28fnz54o",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this specific case, sometimes the action gives a reward of 0, most of the time a reward of 10, but a reward of -10 is also quite likely. This cannot be modelled by a \"classical reinforcement\" learner. For instance, if we just take the mean, we would think that this action yields a much lower reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N8q9yUnTz54o",
    "outputId": "eefe0e01-e66d-4b91-e1be-eb628ddda515",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.mean(ob_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDyEO6g-z54o",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Furthermore, incorporated within this distribution is a risk-reward trade-off. Sometimes taking this action may yield +10, but it may sometimes also lead to a catastrophic failure (i.e. if we get -10). The \"classical\" agent may not infer this from a simple *mean* reward of 3.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7dnsS1wz54o",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Selecting an Action: Collapse of the Reward Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0xbfm8Jz54o",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have been implementing an updating procedure for one single action. Let us now consider consider a hypothetical scenario where we play a game with 3 actions. The reward structures that the agent received under some policy $\\pi$ are defined as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8psJJ56z54o",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ob_rewards1 = [10,10,10,0,1,0,-10,-10,10,10]\n",
    "ob_rewards2 = [5,5,5,1,5,4,5,5,5,5]\n",
    "ob_rewards3 = [-4,-3,-4,0,-2,-4,-4,-3,-5,-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcRXdqXBz54p",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For each of the actions, we defined a prior. As before, our prior will be a simple, uniform distribution with a support of 51. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AQ_q71uz54p",
    "outputId": "88222fe1-cd70-4191-ebdf-2a495c03c98f",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "probabilities = np.ones(51)\n",
    "prior = probabilities / probabilities.sum()\n",
    "prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zimpih-Xz54p",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can now define three priors over the actions spaces, $Z_{1}$, $Z_{2}$ and $Z_{3}$ - three distributions we need to update. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOm8VJrRz54p",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Z1, Z2, Z3 = prior, prior, prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKKxmhVVz54p",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As before, we can loop over the update function. Now, however, we do this for each of the three actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLc6N6DKz54p",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(ob_rewards1)):\n",
    "    Z1 = update_dist(ob_rewards1[i], support, Z1, vmin,vmax, gamma=0.7)\n",
    "    Z2 = update_dist(ob_rewards2[i], support, Z2, vmin,vmax, gamma=0.7)\n",
    "    Z3 = update_dist(ob_rewards3[i], support, Z3, vmin,vmax, gamma=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JuP7pXDQz54q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us visualize three distributions for the 3 actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YX-lse7Jz54q",
    "outputId": "4b47c177-2267-4c5b-dac4-b9fc6d4e3621",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20,5) \n",
    "\n",
    "#The below code will create two plots. The parameters that .subplot take are (row, column, no. of plots).\n",
    "plt.subplot(1,3,1)\n",
    "action1 = plt.bar(support, Z1, color = 'red')\n",
    "plt.title(f'Action 1, E: {np.round(support @ np.array(Z1), 2)}')\n",
    "plt.subplot(1,3,2)\n",
    "action2 = plt.bar(support, Z2, color = 'green')\n",
    "plt.title(f'Action 2, E: {np.round(support @ np.array(Z2), 2)}')\n",
    "plt.subplot(1,3,3)\n",
    "action3 = plt.bar(support, Z3, color = 'purple')\n",
    "plt.title(f'Action 3, E: {np.round(support @ np.array(Z3), 2)}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HRGIvJ9Az54q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As can be seen, we have 3 distributions. Each of these corresponds to the estimated distribution of rewards. In other words, based on $n$ observations, taking the action $A_{1}$ yields $Z_{1}$, $A_{2}$ yields $Z_{2}$ and $A_{3}$ yields $Z_{3}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PLM7TeHz54q",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because we are working with distributions over the action space, let us combine $Z_{1}$, $Z_{2}$ and $Z_{3}$ into a 3 by 51 array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdVjlGwJz54q",
    "outputId": "f9f797e4-59a2-4b70-b499-9ea4a87e6547",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dist = np.array([Z1, Z2, Z3])\n",
    "dist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFixgkOuz54q",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Selecting an action $a'$ from the set of possible actions on the next step can be formalised as follows:\n",
    "\n",
    "$$\\large \\underset{a' \\in \\mathcal{A}}{\\text{argmax}} \\mathbb{ E } [Z(x', a')].$$\n",
    "\n",
    "Thus, we select the action that maximizes the expected value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D57p5VK3z54r",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 7:** Write a piece of code that selects, from these three actions (i.e. ``Z1, Z2, Z3``), the best one. \n",
    "\n",
    "- Hint 1: As discussed in the beginning, the expectation can be computed as the dot product of the support vector and its corresponding probabilities ``expected_value = probs @ outcomes``.\n",
    "- Hint 2: First compute the expectation for each of the actions, then select the action with the highest reward.\n",
    "- Hint 3: Use the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTdNkUr1z54r",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_action(dist,support):\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU7uslepz54r",
    "outputId": "1659a98b-09f6-4154-edc3-bbc4cb7a6b3d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "get_action(dist,support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTr2ma4Yz54r",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As can be seen, based on the distributions, we should select action 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v55r0M1z54r",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distributional Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will now implement an actual game, Atari Freeway. As said before, the goal of this game is the run across a highway and gain as many points as possible. If you get hit by a car, you are being pushed back. If you reach the other side, you will get a reward of 1. Watch a couple of minutes of the clip below for a visual demonstration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RERk15K-z54r",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/freeway.gif\" width=\"750\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TkPFKDbz54s",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will now have a look at a Distributional DQN implementation of the Atari Freeway Game. We will start with a couple of functions. Some are given, and some require you to complete a line of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSP3hWK2z54s",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To simulate the environment, we will make use of OpenAI ``gym``. Have a look at the actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "he9A0Ng_z54s",
    "outputId": "0488c5f9-3564-4c6b-8a40-0e04a475b7dd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('Freeway-ram-v0') \n",
    "aspace = 3                          # action space size\n",
    "env.env.get_action_meanings()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gj04XWzz54s",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us now set the other hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0rNvUMaz54s",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vmin,vmax = -10,10\n",
    "replay_size = 200\n",
    "batch_size = 50\n",
    "N = 51\n",
    "gamma = 0.1\n",
    "episodes = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2HLmMxOz54s",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us now initialise the network parameters. Note that we did not include the network in this notebook. If you are interested you can check out the network in the ``dist_dqn.py``-file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngJsS-Zkz54t",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 8 (Bonus):** Run the trainer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wb1qMnF-z54t",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can now train the model by calling the trainer function. This function returns $\\theta$, ``theta``, the parameters of the neural network and the ``losses``, which are the losses over time. The exact workings of this function are beyond the scope of this session, but you are encouraged to check out the code yourself (to be found in the ``trainer.py`` file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgB5l9ibz54t",
    "outputId": "3a3e0cb9-1f60-4d86-edc8-3610419345d7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "theta, losses = trainer(env, aspace, N, vmin, vmax, gamma, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDrkA9JLz54t",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us now visualize the loss over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-dLqQVdz54t",
    "outputId": "444d5092-2893-45ae-9e5a-6cb26a3f8d4a",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(losses) \n",
    "plt.title('Cross Entropy Loss over Time for the Atari Freeway Agent')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Episode');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOxGHqKGz54u",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can also observe how the agent is doing. Run the cell below to generate a visualization of the DistQN agent playing a game of Atari Freeway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC2GUTW_z54u",
    "outputId": "33188aa3-b376-490d-bfb0-266b6f8afcc5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# create empty list, to be populated by frames\n",
    "clip = []\n",
    "\n",
    "# reset the game\n",
    "state = preproc_state(env.reset())\n",
    "for i in range(1000):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Rendering: {(i/1000)*100}% complete...\")\n",
    "    \n",
    "    # make a prediction for a given state\n",
    "    pred = dist_dqn(state,theta,3)\n",
    "    # get the image\n",
    "    image = get_dist_plot(env,pred,support,shape=(210,160,3))\n",
    "    #pred = pred.detach.numpy()\n",
    "    # select best action\n",
    "    action = get_action_dqn(pred.unsqueeze(dim=0).detach().numpy(),support).item()\n",
    "    # get state, reward and information\n",
    "    s,r,d,j = env.step(action)\n",
    "    # preprocess new state\n",
    "    state = preproc_state(s)\n",
    "    # append individual frames\n",
    "    clip.append(image)\n",
    "    \n",
    "print('Rendering complete! Converting to mp4...')\n",
    "\n",
    "# write to mp4\n",
    "imageio.mimwrite('img/AtariHighway-v3.mp4', clip, fps = 20)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pEfY-KFz54u",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Question 9 (Bonus):** Consider the clip below. Explain the behaviour of the agent using the distribution on the right-hand side of the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4acvpvbiz54u",
    "outputId": "0c6811a5-d5bc-4614-9d54-e17f4619be50",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<video width='600' height='500' controls>\n",
    "  <source src='img/AtariHighway-v2.mp4' type='video/mp4'>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UX2hpRpRz54v",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References \n",
    "\n",
    "- [A Distributional Perspective on Reinforcement Learning, Bellemare et.al. (2017)](https://arxiv.org/pdf/1707.06887.pdf)\n",
    "- *Deep Reinforcement Learning in Action*, Zai & Brown 2020.\n",
    "- [Deep Reinforcement Learning in Action, Zai & Brown, Github Repository](https://github.com/DeepReinforcementLearning/DeepReinforcementLearningInAction)\n",
    "- [Distributional Reinforcement Learning, Bellemare, Dabney and Rowland (2022, draft)](https://www.distributional-rl.org)\n",
    "- [Distributional Reinforcement Learning with Quantile Regression, Dabney et.al. (2017)](https://arxiv.org/abs/1710.10044)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "RL-Summer-School_Session_Dist_RL-(Functions_Excluded).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
