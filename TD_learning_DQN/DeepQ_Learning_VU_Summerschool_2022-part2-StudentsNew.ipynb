{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7549b86",
   "metadata": {},
   "source": [
    "# Reinforcement learning summer school at the VU - 2022 - Workbook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb46de00",
   "metadata": {},
   "source": [
    "## Workshop tutorial, day 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55dd0d8",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning Agent (Part 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5dc2ee",
   "metadata": {},
   "source": [
    "### Author: Buelent Uendes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879505c9",
   "metadata": {},
   "source": [
    "In this notebook, we will use a function approximator to solve the mountain car game. As seen in the previous notebook, a simple agent that uses Q learning can learn to move the car in a way to move up the hill. Yet, for this to work, one had to discretize the state space. However, for large problems this approach is not feasible, given the fact that we then have a state,action pair matrix. To overcome this, we will use a Neural Network that will approximate the state, pair values. For this, we will use PyTorch. If you have not used PyTorch yet, do not worry, as most of the code will be provided for you. Also, you can always ask any TA for further help. Yet, if you want to have a more in-depth tutorial in PyTorch, you can use the following YouTube tutorial:\n",
    "\n",
    "- https://www.youtube.com/watch?v=c36lUUr864M\n",
    "\n",
    "Deep reinforcement learning got popular following the paper published in 2013 [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). Following this paper, several additional techniques were introduced that aim to stabilize the learning process. In the following notebook, we will look at two new methods, memory replay and target networks and will try to solve the mountain car environment using a Deep Reinforcement Learning algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ef6ad",
   "metadata": {},
   "source": [
    "**The code used in this notebook is based upon the implementation of a Deep Q agent as shown in this [tutorial.](https://www.youtube.com/watch?v=NP8pXZdU-5U)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56e0072",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "\n",
    "In the notebook, you will see a couple of ToDos with some instructions. Try your best to work through them and to complete the notebook. In case you run into problems, do not hesitate to ask any of the TAs for help! :) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5bd33c",
   "metadata": {},
   "source": [
    "## Preliminaries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c663abf",
   "metadata": {},
   "source": [
    "### Import main libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56bb551",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gym==0.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d44435",
   "metadata": {},
   "source": [
    "### Seeting the seed for reproducibility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efac6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "seed = 7\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b8108",
   "metadata": {},
   "source": [
    "## General notes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1f4fab",
   "metadata": {},
   "source": [
    "We will introduce the concept of Deep Reinforcement Learning in **three** steps:\n",
    "\n",
    "1) First introduce how to implement a simple deep neural network that is represents an essential building block of Deep Q learning\n",
    "\n",
    "2) Introduce the topic of experience replay/replay buffer\n",
    "\n",
    "3) Introduce the concept of a target network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f29625",
   "metadata": {},
   "source": [
    "## Part 1: Deep neural network and its general characteristics in the context of reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01122b51",
   "metadata": {},
   "source": [
    "### General characteristics of Deep Q learning\n",
    "\n",
    "In the simplest approach, a Deep RL algorithm is:\n",
    "\n",
    "- Episodic (the agent acts in the environment only for a specific number of timesteps)\n",
    "- Online (we train the algorithm while the agent interacts with the environment)\n",
    "- Model-free. We do not attempt to model the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc3b72",
   "metadata": {},
   "source": [
    "In the following, we will implement a deep neural network using the PyTorch library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48f525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        env = environment that the agent needs to play\n",
    "        learning_rate = learning rate used in the update\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        super(DQN,self).__init__()\n",
    "        input_features = env.observation_space.shape[0]\n",
    "        action_space = env.action_space.n\n",
    "        \n",
    "        '''\n",
    "        ToDo: \n",
    "        Write the layers of your neural network! \n",
    "        Make sure that the input features and the output features are in line with the environment that \n",
    "        the class takes as an input feature\n",
    "        '''\n",
    "        #Solution:\n",
    "        \n",
    "        \n",
    "        #Here we use ADAM, but you could also think of other algorithms such as RMSprob\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr = learning_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        x = observation\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        ToDo: \n",
    "        Write the forward pass! You can use any activation function that you want (ReLU, tanh)...\n",
    "        Important: We want to output a linear activation function as we need the q-values associated with each action\n",
    "    \n",
    "        '''\n",
    "        \n",
    "        #Solution:\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32692ba5",
   "metadata": {},
   "source": [
    "That's it! This is the implementation of a deep neural network in PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cc5db9",
   "metadata": {},
   "source": [
    "## Part 2: Experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618bcd95",
   "metadata": {},
   "source": [
    "In a normal implementation of a deep neural network, one would train the algorithm using some sort of a gradient method. Yet, one of the key assumption is that the data is iid, i.e. independent identically distributed which does not hold in our reinforcement learning setting. The next state and its reward depends on the action our agent took the preceeding state which makes subsequent states and the data highly correlated. This can cause the DQN to be instable. To circumvent this, people use in practice a so-called experience replay technique. The main rationale behind this idea is to break the correlation between subsequent transitions by saving experiences in memory and sample randomly from the stored transitions when performing a Q-value update. This 'trick' is essential to make the method work!\n",
    "\n",
    "In the following, we will create a experience replay class that will store the transitions of the deep Q agent. It is important to keep in mind that the replay buffer has a fixed capacity. If the data that we want to store in the replay buffer exceeds the buffer, we want to store only the most recent transitions in the buffer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    \n",
    "    def __init__(self, env, buffer_size, min_replay_size = 1000, seed = 123):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        env = environment that the agent needs to play\n",
    "        buffer_size = max number of transitions that the experience replay buffer can store\n",
    "        min_replay_size = min number of (random) transitions that the replay buffer needs to have when initialized\n",
    "        seed = seed for random number generator for reproducibility\n",
    "        '''\n",
    "        self.env = env\n",
    "        self.min_replay_size = min_replay_size\n",
    "        self.replay_buffer = deque(maxlen=buffer_size)\n",
    "        self.reward_buffer = deque([-200.0], maxlen = 100)\n",
    "        \n",
    "        print('Please wait, the experience replay buffer will be filled with random transitions')\n",
    "                \n",
    "        obs, _ = self.env.reset(seed=seed)\n",
    "        for _ in range(self.min_replay_size):\n",
    "            action = env.action_space.sample()\n",
    "            new_obs, rew, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            transition = (obs, action, rew, done, new_obs)\n",
    "            self.replay_buffer.append(transition)\n",
    "            obs = new_obs\n",
    "    \n",
    "            if done:\n",
    "                obs, _ = env.reset(seed=seed)\n",
    "        \n",
    "        print('Initialization with random transitions is done!')\n",
    "      \n",
    "          \n",
    "    def add_data(self, data): \n",
    "        '''\n",
    "        Params:\n",
    "        data = relevant data of a transition, i.e. action, new_obs, reward, done\n",
    "        '''\n",
    "        self.replay_buffer.append(data)\n",
    "            \n",
    "    def sample(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        batch_size = number of transitions that will be sampled\n",
    "        \n",
    "        Returns:\n",
    "        tensor of observations, actions, rewards, done (boolean) and next observation \n",
    "        '''\n",
    "        \n",
    "        transitions = random.sample(self.replay_buffer, batch_size)\n",
    "        observations = np.asarray([t[0] for t in transitions])\n",
    "        '''\n",
    "        ToDo:\n",
    "        Do the same for the remaining variables and store these as np arrays!\n",
    "        '''\n",
    "\n",
    "        #PyTorch needs these arrays as tensors!\n",
    "        observations_t = torch.as_tensor(observations, dtype = torch.float32)\n",
    "        actions_t = torch.as_tensor(actions, dtype = torch.int64).unsqueeze(-1)\n",
    "        rewards_t = torch.as_tensor(rewards, dtype = torch.float32).unsqueeze(-1)\n",
    "        dones_t = torch.as_tensor(dones, dtype = torch.float32).unsqueeze(-1)\n",
    "        new_observations_t = torch.as_tensor(new_observations, dtype = torch.float32)\n",
    "        \n",
    "        return observations_t, actions_t, rewards_t, dones_t, new_observations_t\n",
    "    \n",
    "    def add_reward(self, reward):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        reward = reward that the agent earned during an episode of a game\n",
    "        '''\n",
    "        \n",
    "        self.reward_buffer.append(reward)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a756de",
   "metadata": {},
   "source": [
    "## Write the code for the vanilla DQN agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b36a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanilla_DQNAgent:\n",
    "    \n",
    "    def __init__(self, env_name, device, epsilon_decay, \n",
    "                 epsilon_start, epsilon_end, discount_rate, lr, buffer_size, seed = 123):\n",
    "        '''\n",
    "        Params:\n",
    "        env = environment that the agent needs to play\n",
    "        device = set up to run CUDA operations\n",
    "        epsilon_decay = Decay period until epsilon start -> epsilon end\n",
    "        epsilon_start = starting value for the epsilon value\n",
    "        epsilon_end = ending value for the epsilon value\n",
    "        discount_rate = discount rate for future rewards\n",
    "        lr = learning rate\n",
    "        buffer_size = max number of transitions that the experience replay buffer can store\n",
    "        seed = seed for random number generator for reproducibility\n",
    "        '''\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(self.env_name, render_mode = None)\n",
    "        self.device = device\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        self.replay_memory = ExperienceReplay(self.env, self.buffer_size, seed = seed)\n",
    "        self.online_network = DQN(self.env, self.learning_rate).to(self.device)\n",
    "        \n",
    "    def choose_action(self, step, observation, greedy = False):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        step = the specific step number \n",
    "        observation = observation input\n",
    "        greedy = boolean that\n",
    "        \n",
    "        Returns:\n",
    "        action: action chosen (either random or greedy)\n",
    "        epsilon: the epsilon value that was used \n",
    "        '''\n",
    "        \n",
    "        epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
    "    \n",
    "        random_sample = random.random()\n",
    "    \n",
    "        if (random_sample <= epsilon) and not greedy:\n",
    "            #Random action\n",
    "            action = self.env.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            #Greedy action\n",
    "            obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "            q_values = self.online_network(obs_t.unsqueeze(0))\n",
    "        \n",
    "            max_q_index = torch.argmax(q_values, dim = 1)[0]\n",
    "            action = max_q_index.detach().item()\n",
    "        \n",
    "        return action, epsilon\n",
    "    \n",
    "    def learn(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        batch_size = number of transitions that will be sampled\n",
    "        '''\n",
    "        \n",
    "        #Sample random transitions with size = batch size\n",
    "        observations_t, actions_t, rewards_t, dones_t, new_observations_t = self.replay_memory.sample(batch_size)\n",
    "\n",
    "        target_q_values = self.online_network(new_observations_t)\n",
    "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "        targets = rewards_t + self.discount_rate * (1-dones_t) * max_target_q_values\n",
    "\n",
    "        #Compute loss\n",
    "        q_values = self.online_network(observations_t)\n",
    "        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "\n",
    "        #Loss\n",
    "        '''\n",
    "        ToDo: \n",
    "        Implement here the loss function! You can choose the standard MSE loss or Huber loss. Call this variable loss!\n",
    "        '''        \n",
    "\n",
    "        \n",
    "        '''\n",
    "        ToDo: Write the gradient descent step, were you optimize the online network based on the loss!'\n",
    "            Tip: You need 3 lines.\n",
    "            1. Call the zero grad method on the self.network optimizer!\n",
    "            2. Call the backward method on the loss\n",
    "            3. Do an optimization step\n",
    "        '''\n",
    "        \n",
    "        #Solution:\n",
    "        #Gradient descent\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eceee3",
   "metadata": {},
   "source": [
    "## Write the training loop and perform the first run!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e78d68",
   "metadata": {},
   "source": [
    "In a last step, we can write a training loop that will put all things together. We will run the training loop for a number of iteration and see how our first algorithm performs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cae3b0",
   "metadata": {},
   "source": [
    "### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0f49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the hyperparameters\n",
    "\n",
    "#Discount rate\n",
    "discount_rate = 0.99\n",
    "#That is the sample that we consider to update our algorithm\n",
    "batch_size = 32\n",
    "#Maximum number of transitions that we store in the buffer\n",
    "buffer_size = 50000\n",
    "#Minimum number of random transitions stored in the replay buffer\n",
    "min_replay_size = 1000\n",
    "#Starting value of epsilon\n",
    "epsilon_start = 1.0\n",
    "#End value (lowest value) of epsilon\n",
    "epsilon_end = 0.05\n",
    "#Decay period until epsilon start -> epsilon end\n",
    "epsilon_decay = 10000\n",
    "\n",
    "max_episodes = 250000\n",
    "\n",
    "#Learning_rate\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d947bf",
   "metadata": {},
   "source": [
    "### Initialize all instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf59524",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vanilla_agent = vanilla_DQNAgent(env_name, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68caa4",
   "metadata": {},
   "source": [
    "### Write a training loop function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad5b08",
   "metadata": {},
   "source": [
    "We will first write a training loop function and let it then run for the vanilla DQN agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c999f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(env_name, agent, max_episodes, target_ = False, seed=42):\n",
    "    \n",
    "    '''\n",
    "    Params:\n",
    "    env = name of the environment that the agent needs to play\n",
    "    agent= which agent is used to train\n",
    "    max_episodes = maximum number of games played\n",
    "    target = boolean variable indicating if a target network is used (this will be clear later)\n",
    "    seed = seed for random number generator for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    average_reward_list = a list of averaged rewards over 100 episodes of playing the game\n",
    "    '''\n",
    "    env = gym.make(env_name, render_mode = None)\n",
    "    env.action_space.seed(seed)\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    average_reward_list = [-200]\n",
    "    episode_reward = 0.0\n",
    "    \n",
    "    for step in range(max_episodes):\n",
    "        \n",
    "        action, epsilon = agent.choose_action(step, obs)\n",
    "       \n",
    "        new_obs, rew, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated        \n",
    "        transition = (obs, action, rew, done, new_obs)\n",
    "        agent.replay_memory.add_data(transition)\n",
    "        obs = new_obs\n",
    "    \n",
    "        episode_reward += rew\n",
    "    \n",
    "        if done:\n",
    "        \n",
    "            obs, _ = env.reset(seed=seed)\n",
    "            agent.replay_memory.add_reward(episode_reward)\n",
    "            #Reinitilize the reward to 0.0 after the game is over\n",
    "            episode_reward = 0.0\n",
    "\n",
    "        #Learn\n",
    "\n",
    "        agent.learn(batch_size)\n",
    "\n",
    "        #Calculate after each 100 episodes an average that will be added to the list\n",
    "                \n",
    "        if (step+1) % 100 == 0:\n",
    "            average_reward_list.append(np.mean(agent.replay_memory.reward_buffer))\n",
    "        \n",
    "        #Update target network, do not bother about it now!\n",
    "        if target_:\n",
    "            \n",
    "            #Set the target_update_frequency\n",
    "            target_update_frequency = 250\n",
    "            if step % target_update_frequency == 0:\n",
    "                dagent.update_target_network()\n",
    "    \n",
    "        #Print some output\n",
    "        if (step+1) % 10000 == 0:\n",
    "            print(20*'--')\n",
    "            print('Step', step)\n",
    "            print('Epsilon', epsilon)\n",
    "            print('Avg Rew', np.mean(agent.replay_memory.reward_buffer))\n",
    "            print()\n",
    "\n",
    "    return average_reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0640da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rewards_vanilla_dqn = training_loop(env_name, vanilla_agent, max_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743ad14c",
   "metadata": {},
   "source": [
    "**Comment**: As you can see, the vanilla deep Q network performs very poorly and does not learn to master the challenge. Play around with the number of iterations and epsilon decay to check if you can improve the algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd015ad5",
   "metadata": {},
   "source": [
    "## Part 3: Target network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9db3c",
   "metadata": {},
   "source": [
    "A problem of the standard Q learning introduced above is the fact that we use the same Q value to choose an action and to evaluate it. To overcome this problem, double-Q learning was proposed in the following paper [Double Q-learning](https://papers.nips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf).\n",
    "In the case of DQN, we can make use of the same idea by training a second neural network,a so-called target network. Just as the name suggests, the target network will be used to compute the target of the update equation using this target network. This target network will only be updated after a pre-defined number of steps to ensure that the target will not move as the DQN network will learn (as it is the case in the standard simple DQN framework). This idea was put forward in the paper again by van Hasselt et al. (2016) [Deep reinforcement learning with double Q-learning](https://arxiv.org/pdf/1509.06461.pdf). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407fc05",
   "metadata": {},
   "source": [
    "Implementing a target network and changing the architecture to a double DQN is rather straightforward. All we need to do is to initialize besides an online network a so-called target network. After a specific number of steps, the parameter values of the target network are reinitialized with the online network after a pre-defined number of steps. For this, we will add a few lines to the vanilla DQN class and call it DDQN (for double deep Q learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d870289",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    \n",
    "    def __init__(self, env_name, device, epsilon_decay, \n",
    "                 epsilon_start, epsilon_end, discount_rate, lr, buffer_size, seed = 123):\n",
    "        '''\n",
    "        Params:\n",
    "        env = name of the environment that the agent needs to play\n",
    "        device = set up to run CUDA operations\n",
    "        epsilon_decay = Decay period until epsilon start -> epsilon end\n",
    "        epsilon_start = starting value for the epsilon value\n",
    "        epsilon_end = ending value for the epsilon value\n",
    "        discount_rate = discount rate for future rewards\n",
    "        lr = learning rate\n",
    "        buffer_size = max number of transitions that the experience replay buffer can store\n",
    "        seed = seed for random number generator for reproducibility\n",
    "        '''\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(self.env_name, render_mode = None)\n",
    "        self.device = device\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.discount_rate = discount_rate\n",
    "        self.learning_rate = lr\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "        self.replay_memory = ExperienceReplay(self.env, self.buffer_size, seed = seed)\n",
    "        self.online_network = DQN(self.env, self.learning_rate).to(self.device)\n",
    "        \n",
    "        '''\n",
    "        ToDo: \n",
    "        Add here a target network and set the parameter values to the ones of the online network! \n",
    "        You can do this in 2 steps:\n",
    "            1) Initialize the target network and call it self.target_network\n",
    "            2) Set the parameters to the ones of the online network\n",
    "               Hint: Use the method 'load_state_dict'!\n",
    "        '''\n",
    "        #WRITE YOUR CODE HERE!\n",
    "        \n",
    "        #Initialize the target network \n",
    "        \n",
    "        #Set the parameters to the ones of the online_network!\n",
    "        \n",
    "    def choose_action(self, step, observation, greedy = False):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        step = the specific step number \n",
    "        observation = observation input\n",
    "        greedy = boolean that\n",
    "        \n",
    "        Returns:\n",
    "        action: action chosen (either random or greedy)\n",
    "        epsilon: the epsilon value that was used \n",
    "        '''\n",
    "        \n",
    "        epsilon = np.interp(step, [0, self.epsilon_decay], [self.epsilon_start, self.epsilon_end])\n",
    "    \n",
    "        random_sample = random.random()\n",
    "    \n",
    "        if (random_sample <= epsilon) and not greedy:\n",
    "            #Random action\n",
    "            action = self.env.action_space.sample()\n",
    "        \n",
    "        else:\n",
    "            #Greedy action\n",
    "            obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "            q_values = self.online_network(obs_t.unsqueeze(0))\n",
    "        \n",
    "            max_q_index = torch.argmax(q_values, dim = 1)[0]\n",
    "            action = max_q_index.detach().item()\n",
    "        \n",
    "        return action, epsilon\n",
    "    \n",
    "    \n",
    "    def return_q_value(self, observation):\n",
    "        '''\n",
    "        Params:\n",
    "        observation = input value of the state the agent is in\n",
    "        \n",
    "        Returns:\n",
    "        maximum q value \n",
    "        '''\n",
    "        #We will need this function later for plotting the 3D graph\n",
    "        \n",
    "        obs_t = torch.as_tensor(observation, dtype = torch.float32)\n",
    "        q_values = self.online_network(obs_t.unsqueeze(0))\n",
    "        \n",
    "        return torch.max(q_values).item()\n",
    "        \n",
    "    def learn(self, batch_size):\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        batch_size = number of transitions that will be sampled\n",
    "        '''\n",
    "        \n",
    "        observations_t, actions_t, rewards_t, dones_t, new_observations_t = self.replay_memory.sample(batch_size)\n",
    "\n",
    "        #Compute targets, note that we use the same neural network to do both! This will be changed later!\n",
    "\n",
    "        target_q_values = self.target_network(new_observations_t)\n",
    "        max_target_q_values = target_q_values.max(dim=1, keepdim=True)[0]\n",
    "\n",
    "        targets = rewards_t + self.discount_rate * (1-dones_t) * max_target_q_values\n",
    "\n",
    "        #Compute loss\n",
    "\n",
    "        q_values = self.online_network(observations_t)\n",
    "\n",
    "        action_q_values = torch.gather(input=q_values, dim=1, index=actions_t)\n",
    "\n",
    "        #Loss, here we take the huber loss!\n",
    "\n",
    "        loss = F.smooth_l1_loss(action_q_values, targets)\n",
    "        \n",
    "        #Uncomment the following code to use the MSE loss instead!\n",
    "        #loss = F.mse_loss(action_q_values, targets)\n",
    "        \n",
    "        #Gradient descent to update the weights of the neural networ\n",
    "        self.online_network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.online_network.optimizer.step()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \n",
    "        '''\n",
    "        ToDO: \n",
    "        Complete the method which updates the target network with the parameters of the online network\n",
    "        Hint: use the load_state_dict method!\n",
    "        '''\n",
    "    \n",
    "        #WRITE YOUR CODE HERE!\n",
    "    \n",
    "\n",
    "    def play_game(self, step=1, seed=123):\n",
    "        \"\"\"\n",
    "        The following method will let the DQNAgent play the game after it has worked \n",
    "        through the number of episodes for training\n",
    "        \"\"\"\n",
    "        \n",
    "        '''\n",
    "        Params:\n",
    "        step = the number of the step within the epsilon decay that is used for the epsilon value of epsilon-greedy\n",
    "        seed = seed for random number generator for reproducibility\n",
    "        '''\n",
    "        #Get the optimized strategy:\n",
    "        done = False\n",
    "        #Reinitialize the game \n",
    "        self.env = gym.make(self.env_name, render_mode='human')\n",
    "        #Start the game\n",
    "        state, _ = self.env.reset()\n",
    "        while not done:\n",
    "            #Pick the best action \n",
    "            action = self.choose_action(step, state, True)[0]\n",
    "            next_state, rew, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated \n",
    "            state = next_state\n",
    "            #Pause to make it easier to watch\n",
    "            time.sleep(0.05)\n",
    "        #Close the pop-up window\n",
    "        self.env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e02d62e",
   "metadata": {},
   "source": [
    "After we have created our DDQNAgent class, we can re-run the experiment from above and see if we can increase the performance! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba258ff0",
   "metadata": {},
   "source": [
    "## Hyperparameters and initialization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d691c9c",
   "metadata": {},
   "source": [
    "Since the hyperparameters are the same as before, we only need to set the new hyperparameter target_update_frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89799b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "dagent = DDQNAgent(env_name, device, epsilon_decay, epsilon_start, epsilon_end, discount_rate, lr, buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef28616",
   "metadata": {},
   "source": [
    "### Main loop DDQN - double deep Q network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63914e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_rewards_ddqn = training_loop(env_name, dagent, max_episodes, target_ = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce395b6",
   "metadata": {},
   "source": [
    "**Comments**:\n",
    "\n",
    "As you can see, implementing a target network improved the performance of the deep reinforcement learning algorithm greatly! \n",
    "\n",
    "We can also plot the results of both algorithms to see the difference even more clearly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10958adb",
   "metadata": {},
   "source": [
    "**Comment**:\n",
    "\n",
    "Here we can plot the results of the algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b71f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1000*(np.arange(len(average_rewards_ddqn))+1),average_rewards_ddqn)\n",
    "plt.plot(1000*(np.arange(len(average_rewards_vanilla_dqn))+1),average_rewards_vanilla_dqn)\n",
    "# specifying horizontal line type\n",
    "plt.axhline(y = -110, color = 'r', linestyle = '-')\n",
    "plt.title('Average reward over the past 100 simulations')\n",
    "plt.xlabel('Number of simulations')\n",
    "plt.legend(['Double DQN', 'Vanilla DQN', 'Benchmark solving the game'])\n",
    "plt.ylabel('Average reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae95e6",
   "metadata": {},
   "source": [
    "As we can see, the Double DQN performs significantly better than the vanilla DQN. The horizontal redline is the benchmark, as one considers the mountain car environment to be solved when the average reward over 100 subsequent trials is -110 ([check this link for further info](https://github.com/openai/gym/wiki/Leaderboard#mountaincar-v0))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6426925",
   "metadata": {},
   "source": [
    "You can play around the hyperparameter and see how the results change if, for example, you lower the discount rate or learning rate! Also, you can see if changing the neural network architecture, i.e. making it deeper, will lead to an increase in performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da636772",
   "metadata": {},
   "source": [
    "## Reap the rewards of the hard work - see the DDQN play the game! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8e811",
   "metadata": {},
   "source": [
    "Now that we worked through two different deep reinforcement learning architectures, we can see the DQN solve the game. The code below with let the DQNAgent play the mountain car game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "dagent.play_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf0672",
   "metadata": {},
   "source": [
    "## Visualize the result in a 3D plot "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed3ad4",
   "metadata": {},
   "source": [
    "We can visualize the result in a 3D plot, plotting the x-position as well as the velocity with the corresponding value function. To recall, the value of a particular state is, in case of a greedy policy, the corresponding maximum state action pair! The following function will plot the value function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d14dbd",
   "metadata": {},
   "source": [
    "The following code will plot the value function that results of the DDQN algorithm in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46794ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "low = env.observation_space.low\n",
    "high =env.observation_space.high\n",
    "\n",
    "bin_size = 20\n",
    "bin_x = np.linspace(low[0], high[0], bin_size)\n",
    "bin_velocity = np.linspace(low[1], high[1], bin_size)\n",
    "\n",
    "X, Y = np.meshgrid(bin_x, bin_velocity)\n",
    "Z = np.zeros((len(X), len(Y)))\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(Y)):\n",
    "        Z[i][j] = dagent.return_q_value([X[0][i], Y[j][0]])\n",
    "fig = plt.figure(figsize =(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "#ax.contour3D(X, Y, Z, 50, cmap='magma')\n",
    "ax.set_xlabel('x-position', fontsize = 18)\n",
    "ax.set_ylabel('velocity', fontsize = 18)\n",
    "ax.set_zlabel('value function', fontsize = 18)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "ax.set_title('Visualization of the value function', fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be254b",
   "metadata": {},
   "source": [
    "**Your turn!**\n",
    "\n",
    "You can play around with the deep reinforcement learning architecture and see what impact for example the discount rate has. Also, you can modify the architecture of the neural network (by making it more deep and/or change the activation function). Just have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1214f5",
   "metadata": {},
   "source": [
    "## Extensions/Interesting notes: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9328c6",
   "metadata": {},
   "source": [
    "Following the sucess of the paper by Minh et al. (2013), research in deep reinforcement learning has progressed and a couple of extensions to the basic framework as well as tricks have been proposed, such as dueling deep Q learning (also called D3QN) and priotized experience replay. But before I give you some pointers on this, we will discuss the deadly triad in a bit more detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487ba3f5",
   "metadata": {},
   "source": [
    "## 1) The deadly triad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a725339",
   "metadata": {},
   "source": [
    "Some of you might have heard of the term 'deadly triad' which refers to the instability a reinforcement learning algorithm faces, when an algorithm makes use of:\n",
    "\n",
    "- function approximation\n",
    "- bootstrapping\n",
    "- off-policy evaluation\n",
    "\n",
    "Our deep reinforcement learning algorithm makes use of all three concepts. Yet, it does **not** state that instability/divergence always occur when all three above-mentioned techniques are used. The deadly triad only states that it **can** occur. An interesting paper that addresses this issue empirically is the following paper by van Hasselt et al. (2018) [Deep Reinforcement Learning and the Deadly Triad](https://arxiv.org/pdf/1812.02648.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb749a4",
   "metadata": {},
   "source": [
    "For doing this, they realize that if one bounds the rewards in the interval between $[-1,1]$, then one can show that the corresponding Q values are bounded given by the following equation:\n",
    "\n",
    "$  \\sum_{t'=t}^{T} \\gamma^{t'-t} |r_{t'}| \\le \\sum_{t'=t}^{\\infty} \\gamma^{t'-t} |r_{t'}| \\le \\sum_{t'=t}^{\\infty} \\gamma^{t'-t} = \\frac{1}{1-\\gamma}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2abe3e",
   "metadata": {},
   "source": [
    "According to this, any Q-value is theoretically bounded by the above equation. In our case, by $100$ (given a discount rate of $0.99$). Hence, if the Q-value exceeds this bound, we say that soft divergence occurs.\n",
    "\n",
    "In their work, they find via running several experiments interesting insights:\n",
    "\n",
    "- If one does not correct for overestimating bias (by for example not using a target network), divergence can occur more frequently.\n",
    "\n",
    "- Increasing the multistep return decreases the chance of divergence.\n",
    "\n",
    "- The effect of the neural network size is not straightforward, as the best performing architectures in their experiment are large, but also tend to show some instabilities.\n",
    "\n",
    "Hence, they suggest that one can prevent instabilities by reducing the overestimation bias and by bootstraping on a separate network (also using multi-step returns). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd48d55",
   "metadata": {},
   "source": [
    "## 2) The importance of the random seed - instability of the DeepRL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128340f",
   "metadata": {},
   "source": [
    "As stressed [in this good post](https://spinningup.openai.com/en/latest/spinningup/spinningup.html#closing-thoughts), the performance of DeepRL algorithms is very sensitive to stochasticity and the particular choice of the hyperparameters chosen. For this reason, one should run any DeepRL algorithm on a number of different random seeds and carefully tune the hyperparameters. This aspect is also discussed in this [paper](https://arxiv.org/pdf/1708.04133.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8b174",
   "metadata": {},
   "source": [
    "In the following, we will show how setting different seeds affects the performance of the double deep Q network. The following code is inspired by the code [here](https://gymnasium.farama.org/tutorials/reinforce_invpend_gym_v26/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd81af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_for_different_seeds = []\n",
    "for seed in range(5): # Here we go for 5 seeds only, as otherwise the code runs for too long!\n",
    "    # Reset the pytorch and numpy seeds\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Initialize the agent:\n",
    "    dagent = DDQNAgent(env_name, device, epsilon_decay, epsilon_start, \n",
    "                       epsilon_end, discount_rate, lr, buffer_size, seed)\n",
    "    \n",
    "    rewards_for_different_seeds.append(training_loop(env_name, dagent, max_episodes, target_ = True)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbd050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe to plot using seaborn library\n",
    "rewards = pd.DataFrame(rewards_for_different_seeds).melt()\n",
    "rewards.rename(columns={\"variable\": \"episodes\", \"value\": \"reward\"}, inplace=True)\n",
    "sns.set(style=\"darkgrid\", context=\"talk\", palette=\"rainbow\")\n",
    "sns.lineplot(x=\"episodes\", y=\"reward\", data=rewards).set(\n",
    "    title=\"Performance of Double DQN for different seeds (MountainCar-v0)\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3dfa99",
   "metadata": {},
   "source": [
    "**Comment**:\n",
    "As expected, the performance of the DDQN algrorithm is heavily affected by the different seeds! This illustrates the importance of running the experiment with different seeds!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dd64e",
   "metadata": {},
   "source": [
    "## 3) Dynamic discount rate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82524f2",
   "metadata": {},
   "source": [
    "One of the hyperparameter that one needs to choose carefully is the discount rate $\\gamma$. In their work, [Francois-Lavet et al. (2016)](https://arxiv.org/pdf/1512.02011.pdf) show that one can increase the performance and significantly decrease the number of learning steps required, by not only having a dynamic $\\epsilon$ rate, but also by having a discount rate that increases over time. In particular, they suggest to:\n",
    "\n",
    "$\\gamma_{k+1} = 1 - 0.98(1- \\gamma_{k}) $\n",
    "\n",
    "The start value of gamma is set to $0.9$ and it increases up to a final value of $0.99$. You could try to implement this and check its effect on the result!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22661023",
   "metadata": {},
   "source": [
    "## 4) Priotized experience replay "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a738bf",
   "metadata": {},
   "source": [
    "As we saw above, an important part of deep reinforcement learning is the experience replay buffer. This concept lets the agent remember and reuse old experiences from the past. Yet, those samples were sampled uniformly. Yet, one could imagine that some experiences are more fruitful for the agent to be replayed than others. Intuitively, one would like to replay experiences for which the agent can learn the most. To do this, one needs to change the sample procedure in a way that experiences for which the agent can learn more have a higher sampling probability. This is the key idea of the priotized experience replay buffer, as introduced by [Schaul et al. (2015)](https://arxiv.org/pdf/1511.05952.pdf). \n",
    "In their work, they propose to measure the expected learning significance by the magnitude of the temporal difference  (TD) error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e830b4",
   "metadata": {},
   "source": [
    "In particular, they propose the following. The priority of transition $p_{i}$ is based on the absolute magnitude of the TD error $\\delta_{i}$ plus a small, positive constant $\\epsilon$ which ensures that all samples have a non-zero probability to be resampled:\n",
    "\n",
    "$p_{i} = |\\delta_{i}| + \\epsilon$\n",
    "\n",
    "Then, the probability of sampling transition $i$ is defined as:\n",
    "\n",
    "$P(i) = \\frac {p_{i}^{\\alpha}}{\\sum_{k} p_{k}^{\\alpha}}$ where $\\alpha$ determines how much priotization is used (with $\\alpha = 1$ representing the standard uniform case).\n",
    "\n",
    "Given that we change the sampling procedure from a uniform one, to a different one in which transitions with a higher TD error have a higher chance of getting resampled, we need to correct for the bias that occurs as stochastic updates rely on the assumption that updates correspond to the same distribution as the expectation. This can be done via importance sampling, i.e. :\n",
    "\n",
    "$w_{i} = (\\frac {1}{N} \\frac{1}{P(i)})^{\\beta}$\n",
    "\n",
    "where $N$ refers to the size of the replay buffer and $\\beta$ to a hyperparameter (with 1 compensating fully for non-uniform probabilities). For stability reasons, they suggest to normalize the weights by dividing all weights by the corresponding largest weight in the buffer.\n",
    "\n",
    "Having this, the only change on needs to do to for the training algorithm is to multiply the gradient in the update equation with $w_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1ff58",
   "metadata": {},
   "source": [
    "## 5) Dueling Deep Reinforcement learning - D3QN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cdf2c4",
   "metadata": {},
   "source": [
    "Another extension to the standard DDQN framework is the idea of dueling deep reinforcement learning. This was introduced by [Wang et al. (2016).](https://arxiv.org/pdf/1511.06581.pdf).\n",
    "\n",
    "The key idea is to note that the Q function can be decomposed into the value function $V(s)$ which denotes how valuable it is for an agent to be in a particular state $s$ and an advantage function $A(s,a)$ which determines how valuable it is to take a particular action $a$ in a state $s$:\n",
    "\n",
    "$Q(s,a) = V(s) + A(s,a)$\n",
    "\n",
    "Yet, given a particular Q function, one can decompose it into the components of $V(s)$ and $A(s,a)$ in many ways which  raises identifiability issues. To circumvent this issue, the authors suggest to force the highest Q value to be equal to the value function, i.e.\n",
    "\n",
    "$Q(s,a) = V(s) + ( A(s,a) - max_{a' \\in |A|}A(s,a'))$\n",
    "\n",
    "Alternatively, one can do it also using the average over all actions:\n",
    "\n",
    "\n",
    "$Q(s,a) = V(s) + A(s,a) - \\frac{1}{||a||}\\sum_{a'}A(s,a')$\n",
    "\n",
    "To implement this in PyTorch, one only needs to seperate the layers into two different streams, where one layer calculates the value function and the other one the advantage function. Lastly, one needs to then aggregate them again (and subtract the average!)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
