{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b0c7f42",
   "metadata": {},
   "source": [
    "# Policy Gradient --- Practical Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from jax import grad\n",
    "jax.config.update('jax_platform_name', 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98bfe8",
   "metadata": {},
   "source": [
    "## Setting up the RL task with Openai Gym\n",
    "\n",
    "Gym (https://www.gymlibrary.ml/) is the de facto standard interface for simulating **Markov Decision Processes** (MDPs).\n",
    "\n",
    "An MDP is made of \n",
    "* a **state space** $\\mathcal{S}$\n",
    "* an **action space** $\\mathcal{A}$\n",
    "* a **starting-state distribution** $\\mu(s)$\n",
    "* a **transition function** $p(s'\\mid s,a)$\n",
    "* a **reward function** $r(s,a)$\n",
    "\n",
    "![title](img/rl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcf9c0b",
   "metadata": {},
   "source": [
    "Let's start by creating an MDP (our **environment** or reinforcement learning **task**).\n",
    "\n",
    "Gym has several ready-to-use tasks (you can also create custom environments by implementing the gym interface)\n",
    "\n",
    "The `make` method accepts a string that specifies the name and version of the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377b11bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02712c",
   "metadata": {},
   "source": [
    "**Cart-Pole** is a standard control task where the goal is to balance a pole that is attached to a cart, only by controlling the horizontal motion of the cart.\n",
    "\n",
    "![title](img/cartpole.gif)\n",
    "\n",
    "Let's have a look at the **state space**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b539db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1befbe",
   "metadata": {},
   "source": [
    "In this MDP, a state is a vector of 4 real numbers. We call this a *continuous* state space.\n",
    "\n",
    "Let's take note of the state dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85335abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f119b",
   "metadata": {},
   "source": [
    "Now let's have a look at the **action space**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15362234",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335850e0",
   "metadata": {},
   "source": [
    "We only have two actions, `0` (push cart to the left) and `1` (push cart to the right). This is a *finite* action space. \n",
    "\n",
    "Let's take note of the number of actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c6575",
   "metadata": {},
   "source": [
    "The **starting-state distribution** is given by the `reset` method. It returns the initial state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb077ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd58c5",
   "metadata": {},
   "source": [
    "Both the **transition function** and the **reward function** are implemented by the `step` method. It takes an action and it returns the next state, the reward, a termination or \"done\" flag and a side-information dictionary (the latter is empty for Cart-Pole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee71426",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818436f3",
   "metadata": {},
   "source": [
    "Note that the environment object is *stateful*: the next state and reward depend on the (internal) current state and the given action.\n",
    "\n",
    "In Cartpole, reward is 1 at every step (if the pole is up, you are doing good)\n",
    "\n",
    "The episode terminates (`step` returns a `True` done flag) when the pole falls outside a certain angle or the cart goes too far to the left or to the right.\n",
    "\n",
    "See the docs for more detailed info on the Cart-Pole task (https://www.gymlibrary.ml/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a61c9",
   "metadata": {},
   "source": [
    "## A simple agent interface\n",
    "\n",
    "We have our environment, now let's build an **agent**. \n",
    "\n",
    "Our agent will `act` depending on the current state and `update` its behavior based on experience (learn).\n",
    "\n",
    "Our default implementation is an agent that does random actions and does not learn anything. This agent has a fixed **stochastic policy**\n",
    "\n",
    "$\\LARGE\\mathbb{P}(A_t=a\\mid S_t=s) = \\pi(a\\mid s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4adeeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    A (learning) agent\n",
    "    \n",
    "    Attributes\n",
    "    seed: random seed (for reproducibility)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed=0):\n",
    "        self.reset(seed=seed)\n",
    "    \n",
    "    def reset(self, seed=0):\n",
    "        \"\"\"\n",
    "        Reset policy to default and re-seed the random number generator\n",
    "        \n",
    "        Parameters\n",
    "        seed: random seed (for reproducibility)\n",
    "        \"\"\"\n",
    "        self.key = random.PRNGKey(seed) #jax RNG bookkeeping (not important)\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        The current policy of the agent.\n",
    "        \n",
    "        Parameters\n",
    "        state: the current state of the MDP\n",
    "        \n",
    "        Returns: the selected action\n",
    "        \"\"\"\n",
    "        self.key, subkey = random.split(self.key) #jax RNG bookkeeping (not important)\n",
    "        action_probs = jnp.ones(n_actions) / n_actions\n",
    "        action = random.choice(subkey, n_actions, p=action_probs).item() # sample (random) action\n",
    "        return action\n",
    "    \n",
    "    def update(self, state, action, discounted_reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update the agent's policy given one piece of interaction data\n",
    "        \n",
    "        Parameters\n",
    "        state: last state in which an action was selected\n",
    "        action: last action selected by the agent\n",
    "        discounted_reward: last reward received by the agent, already discounted\n",
    "        next_state: state resulting from the agent's last action\n",
    "        done: True if next_state is a terminal state\n",
    "        \n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab9f34a",
   "metadata": {},
   "source": [
    "## The interaction loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef54d0cf",
   "metadata": {},
   "source": [
    "The following function makes the agent interact with the environment over several episodes and records the **performance** of the agent at each episode\n",
    "\n",
    "$\\LARGE J(\\pi)=\\mathbb{E}_{S_0\\sim\\mu, A_t\\sim\\pi(\\cdot|S_t),S_{t+1}\\sim p(\\cdot|S_t,A_t)}\\left[\\sum_{t=0}^\\infty\\gamma^t R_{t+1}\\right]$\n",
    "\n",
    "We can use *random seeds* to make the experiment fully reproducible. \n",
    "\n",
    "The code for randomization is already here. If you want to know more about random numbers in Jax see https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#jax-prng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca87f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interaction(env, agent, n_steps=10**5, discount=0.99, seed=None, render=False):\n",
    "    \"\"\"\n",
    "    Simulates several episodes of interaction between an MDP and an (learning) agent\n",
    "    \n",
    "    Parameters\n",
    "    env: the MDP\n",
    "    agent: the agent\n",
    "    n_steps: total number of interaction time-steps (may result in multiple episodes)\n",
    "    seed: random seed\n",
    "    render: if True, the interaction is displayed graphically\n",
    "    \n",
    "    Returns: a list with the agent's performance for each episode\n",
    "    \"\"\"    \n",
    "    performance = 0.\n",
    "    performances = [] #we will store the performance of each episode\n",
    "    state = env.reset(seed=seed) #sample initial state\n",
    "    last_reset = 0 #starting timestep of the current episode\n",
    "    episode = 1 #index of current episode\n",
    "    \n",
    "    for t in range(n_steps): #interaction loop\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        discounted_reward = reward * discount**(t - last_reset) #discounting is from the start of the episode\n",
    "        performance += discounted_reward\n",
    "        \n",
    "        #Display interaction\n",
    "        if render:\n",
    "            env.render()\n",
    "            \n",
    "        #Update the agent's policy with the latest piece of interaction data\n",
    "        agent.update(state, action, discounted_reward, next_state, done)\n",
    "        \n",
    "        if done: #go to next episode\n",
    "            print('Episode %d: performance = %.2f' % (episode, performance))\n",
    "            episode += 1\n",
    "            performances.append(performance)\n",
    "            performance = 0\n",
    "            state = env.reset()\n",
    "            last_reset = t + 1\n",
    "        else: #go to next state\n",
    "            state = next_state\n",
    "        \n",
    "    env.close()\n",
    "    return performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9218f",
   "metadata": {},
   "source": [
    "Let's simulate the interaction of the random agent with the cartpole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = Agent(seed=0)\n",
    "performances = run_interaction(env, random_agent, n_steps=1000, render=False, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb0c03",
   "metadata": {},
   "source": [
    "Let's plot the performance over the episodes. This is called a **learning curve**, but this agent cannot learn anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d76124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #matlab-like plots https://matplotlib.org/stable/tutorials/introductory/pyplot.html\n",
    "\n",
    "plt.plot(performances)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f166e4d4",
   "metadata": {},
   "source": [
    "## Policy Gradient: theory recap\n",
    "\n",
    "We fix a parametric policy class\n",
    "\n",
    "$\\LARGE \\Pi=\\{\\pi_\\theta\\mid\\theta\\in\\Theta\\}$\n",
    "\n",
    "We use the following shorthand for the performance measure:\n",
    "\n",
    "$\\LARGE J(\\theta) = J(\\pi_\\theta)$\n",
    "\n",
    "And learn policy parameters by **gradient ascent**\n",
    "\n",
    "$\\LARGE \\theta\\gets\\theta+\\alpha\\nabla_\\theta J(\\theta)$\n",
    "\n",
    "where $\\alpha$ is the **learning rate** and the expression of the gradient is given by the **policy gradient theorem**\n",
    "\n",
    "$\\Large\\nabla_\\theta J(\\theta) = (1-\\gamma)^{-1}\\mathbb{E}_{S\\sim d_\\theta, A\\sim\\pi_\\theta(\\cdot| S)}\\left[\\nabla_\\theta\\log\\pi_\\theta(A| S)Q^\\theta(S,A)\\right]$\n",
    "\n",
    "where the state-action value function is:\n",
    "\n",
    "$\\LARGE Q^\\theta(s,a) = \\mathbb{E}_\\theta\\left[\\sum_{t=0}^\\infty\\gamma^t R_{t+1}\\bigg|S_0=s, A_0=a\\right]$\n",
    "\n",
    "and the discounted state-occupancy measure is:\n",
    "\n",
    "$\\LARGE d_\\theta(s)=(1-\\gamma)\\sum_{t=0}^\\infty\\gamma^t\\mathbb{P}_\\theta(S_t=s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62dacdc",
   "metadata": {},
   "source": [
    "## JAX quickstart\n",
    "\n",
    "JAX is a numerical computing and automatic differentiation library.\n",
    "\n",
    "The basic objects are (multi-dimensional) arrays (same as in *numpy*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562ca0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp #same as numpy!\n",
    "\n",
    "v = jnp.array([0.3, 0.2, 0.5]) #a 1-D array (a vector)\n",
    "A = jnp.array([[1.,-1.,0],[0,1.,1.]]) # a 2-D array (a matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16886cd2",
   "metadata": {},
   "source": [
    "We can access arrays for reading values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97185205",
   "metadata": {},
   "outputs": [],
   "source": [
    "v[1] #second element of the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e47e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0,1] #matrix element in first row and second column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13011f",
   "metadata": {},
   "source": [
    "For advanced array reading, see slicing in numpy https://www.w3schools.com/python/numpy/numpy_array_slicing.asp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea4b31e",
   "metadata": {},
   "source": [
    "**Important**: jax arrays are *immutable*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb71b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    v[1] = 0. #attempt to assign new value to array element\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753e4e5",
   "metadata": {},
   "source": [
    "You must create a new array instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96878e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = v.at[1].set(0.)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb1a91",
   "metadata": {},
   "source": [
    "We can perform linear algebra on arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dfe96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elementwise operations (the result is always a new array)\n",
    "print(v+v)\n",
    "print(2*v)\n",
    "print(v*v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0da07",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ v #matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a90044",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T #matrix transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9600048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.dot(v,v) #dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d6f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T #matrix transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d690c9e",
   "metadata": {},
   "source": [
    "We can easily **differentiate** functions with `grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "def f(x):\n",
    "    return jnp.dot(x, x)\n",
    "\n",
    "nabla_f = grad(f) #gradient of f w.r.t. x (itself a function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa580ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nabla_f(v) #the gradient of f evaluated at x=v (the gradient of x^Tx is 2x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4829dc1",
   "metadata": {},
   "source": [
    "Finally, some useful jax functions (you may need them for the exercises)\n",
    "\n",
    "* `jnp.dot` : dot product https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.dot.html\n",
    "* `jnp.log` : natural logarithm https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.log.html\n",
    "* `jnp.exp` : exponential https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.exp.html\n",
    "* `jnp.sum` : sum of array elements https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.sum.html\n",
    "* `jnp.cumsum` : cumulative sum https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.cumsum.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04689ef",
   "metadata": {},
   "source": [
    "## EXERCISE 1: Softmax policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708dfa77",
   "metadata": {},
   "source": [
    "It's your turn to implement an agent that acts according to a parametric policy\n",
    "\n",
    "The **Softmax policy** is often used for finite actions. A possible formulation is:\n",
    "\n",
    "$\\Huge \\pi_\\theta(a\\mid s) = \\frac{\\exp(\\theta_a\\cdot s)}{\\sum_{a'\\in\\mathcal{A}}\\exp(\\theta_{a'}\\cdot s)}$\n",
    "\n",
    "where the policy parameter $\\theta$ is a 2-D array (matrix) of size n_actions $\\times$ state_dim and $\\theta_a$ denotes the row of $\\theta$ corresponding to action $a$. \n",
    "\n",
    "This is called a *linear* softmax policy because, for each action, the (unnormalized) log-probability is given by a linear combination of $\\theta_a$ and the current state.\n",
    "\n",
    "To implement the softmax policy, you just need to implement the `policy` method in the code below, which returns the probability of playing each action given the policy parameters and the state.\n",
    "\n",
    "*Suggestion: implement the log-action probabilities first in the* `log_policy` *method*\n",
    "\n",
    "Note that, even if the SoftmaxAgent object stores the current policy parameters as `self.policy_parameters`, the `policy` and `log_policy` methods take parameters as an input. This is to allow differentiation with JAX in the following steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAgent(Agent):\n",
    "    \"\"\"\n",
    "    Agent implementing a fixed linear-Softmax policy.\n",
    "    \n",
    "    Attributes\n",
    "    state_dim: dimension of state vector\n",
    "    n_actions: number of available actions\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, n_actions, seed=0):\n",
    "        self.state_dim = state_dim\n",
    "        self.n_actions = n_actions\n",
    "        self.reset(seed=seed)\n",
    "        \n",
    "    def reset(self, seed=0):\n",
    "        \"\"\"Set policy parameters to all zeros and re-seed random number generator\"\"\"\n",
    "        self.key = random.PRNGKey(seed)\n",
    "        self.policy_parameters = jnp.zeros((self.n_actions, self.state_dim))\n",
    "    \n",
    "    def log_policy(self, policy_parameters, state):\n",
    "        \"\"\"\n",
    "        Logarithm of action probability given policy parameters and state\n",
    "        \n",
    "        Parameters\n",
    "        policy_parameters: parameters (theta) of the policy to evaluate\n",
    "        state: the current state\n",
    "        \n",
    "        Returns\n",
    "        a vector of log-probabilities, one for each action\n",
    "        \"\"\"\n",
    "        state_features = jnp.array(state) #let's make sure the state is a jax array\n",
    "        \n",
    "        #YOUR SOLUTION HERE\n",
    "    \n",
    "    def policy(self, policy_parameters, state):\n",
    "        \"\"\"\n",
    "        Action probability given policy parameters and state\n",
    "        \n",
    "        Parameters\n",
    "        policy_parameters: parameters (theta) of the policy to evaluate\n",
    "        state: the current state\n",
    "        \n",
    "        Returns\n",
    "        a vector of probabilities, one for each action\n",
    "        \"\"\"\n",
    "        #YOUR SOLUTION HERE\n",
    "    \n",
    "    def act(self, state):\n",
    "        self.key, subkey = random.split(self.key)\n",
    "        action_probs = self.policy(self.policy_parameters, state)\n",
    "        action = random.choice(subkey, n_actions, p=action_probs).item()\n",
    "        return action\n",
    "    \n",
    "    def score_function(self, policy_parameters, state, action):\n",
    "        \"\"\"\n",
    "        Gradient (w.r.t. policy parameters) of log-probability of a fixed action, given policy parameters and state\n",
    "        \n",
    "        Parameters:\n",
    "        policy_parameters: parameters (theta) of the policy to evaluate\n",
    "        state: the current state\n",
    "        action: the selected action\n",
    "        \n",
    "        Returns: the gradient (it has the same shape as self.policy_parameters) \n",
    "        \"\"\"\n",
    "        #partial solution, only the shape is correct. REPLACE WITH YOUR SOLUTION\n",
    "        return jnp.zeros(self.policy_parameters.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab38281",
   "metadata": {},
   "source": [
    "The Softmax policy with all parameters set to zero is again the random policy! To test your implementation, you can inspect the action probabilities it produces and/or run it in the Cart-Pole task using `run interaction`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d1b96",
   "metadata": {},
   "source": [
    "## EXERCISE 2: Score function\n",
    "\n",
    "To compute the policy gradient, we just need to differentiate the log-action-probability. This gradient is called **score function**\n",
    "\n",
    "$\\Huge \\nabla_\\theta\\log\\pi_\\theta(a|s)$\n",
    "\n",
    "A stub of the `score_function` method is given.\n",
    "\n",
    "**Hint 1:** if you haven't implemented the `log_policy` method yet, it is a good time to do it!\n",
    "\n",
    "**Hint 2:** `log_policy` has an another argument than the policy parameters (the state). By default, `grad` differentiates w.r.t. the *first* function argument\n",
    "\n",
    "**Hint 3:** `log_policy` returns a vector of probabilities, one for each action, but `grad` can only differentiate scalar-valued functions. You may want to index the probability vector *before* differentiating. JAX encourages a functional-programming style: use `lambda` or nested functions to define exactly what you want to differentiate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7701a6ef",
   "metadata": {},
   "source": [
    "## EXERCISE 3: REINFORCE\n",
    "\n",
    "Finally, we need to estimate the policy gradient from data. The simplest way is given by the REINFORCE estimator (here given in the \"PGT\" variant):\n",
    "\n",
    "$\\LARGE \\widehat{\\nabla}_\\theta J(\\theta)=\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(A_t\\mid S_t)\\sum_{h=t}^{T-1}\\gamma^h R_{h+1}$\n",
    "\n",
    "Implement the gradient ascent update in the `update` method using the REINFORCE gradient estimator and the `score_function` method you have implemented before.\n",
    "\n",
    "You may need to play with the `learning_rate` parameter. A larger learning rate can lead to faster convergence but also to instabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1086e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGSoftmaxAgent(SoftmaxAgent):\n",
    "    \"\"\"\n",
    "    Agent learning a linear-Softmax policy with REINFORCE\n",
    "    \n",
    "    Attributes\n",
    "    state_dim: dimension of state vector\n",
    "    n_actions: number of available actions\n",
    "    discount: discount factor of rewards\n",
    "    learning_rate: learning rate of the gradient-ascent update (alpha)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, n_actions, seed=0, discount=0.99, learning_rate=1e-3):\n",
    "        super().__init__(state_dim, n_actions, seed)\n",
    "        self.discount = discount\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def reset(self, seed=0):\n",
    "        super().reset(seed=seed)\n",
    "        #bookkeeping:\n",
    "        self.discounted_rewards = [] #will store discounted rewards for each timestep, just for current episode\n",
    "        self.scores = [] #will store scores (grad-log-pi) for each timestep, just for current episode\n",
    "    \n",
    "    def update(self, state, action, discounted_reward, next_state, done):\n",
    "        #bookkeeping:\n",
    "        current_param = self.policy_parameters\n",
    "        self.discounted_rewards.append(discounted_reward)\n",
    "        score = self.score_function(current_param, state, action) #calls the method you have implemented in Ex. 2\n",
    "        self.scores.append(score)\n",
    "        \n",
    "        if done:\n",
    "            #At the end of each episode, update policy parameters with estimated policy gradient            \n",
    "            #YOUR SOLUTION HERE\n",
    "            \n",
    "            #bookkeping: data are discarded after each policy update\n",
    "            self.discounted_rewards = []\n",
    "            self.scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3710b5",
   "metadata": {},
   "source": [
    "Time to test you learning agent! You should see a small improvement w.r.t. the random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81028493",
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforce_agent = PGSoftmaxAgent(state_dim, n_actions, seed=0, learning_rate=1e-3)\n",
    "performances = run_interaction(env, reinforce_agent, n_steps=10000, render=False)\n",
    "plt.plot(performances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e926d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(performances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce9d35b",
   "metadata": {},
   "source": [
    "You should notice a slight increase in the performance...but what behavior has the agent actually learned? It is a good practice to render the behavior of the learned policy. You should compare it with the random policy that was defined at the beginning of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "env = gym.make(\"CartPole-v1\") #create a new environment to avoid pygame display errors\n",
    "run_interaction(env, random_agent, n_steps=200, render=True, seed=0); #display behavior of random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "env = gym.make(\"CartPole-v1\") #create a new environment to avoid pygame display errors\n",
    "run_interaction(env, reinforce_agent, n_steps=500, render=True, seed=0); #display behavior of learned policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76ecd9",
   "metadata": {},
   "source": [
    "## Extras\n",
    "\n",
    "There are several ways you can improve your learning agent!\n",
    "\n",
    "### Computational efficiency\n",
    "\n",
    "* Jax offers many ways to speed up computations https://jax.readthedocs.io/en/latest/notebooks/quickstart.html#using-jit-to-speed-up-functions\n",
    "* The action sampling of the Softmax policy can be made more efficient using the Gumbel trick https://timvieira.github.io/blog/post/2014/07/31/gumbel-max-trick/. The Gumbel distribution is already available in `jax.random` https://jax.readthedocs.io/en/latest/_autosummary/jax.random.gumbel.html\n",
    "\n",
    "### Variance reduction\n",
    "* Variance of the estimated gradients is one of the main drawbacks of policy gradient algorithms. A first way to reduce variance is to increase the **batch size**. You just need to collect $N>1$ trajectories for each policy update (e.g. $N=100$) and then average the $N$ gradient estimates. Here superscripts are used to index trajectories.\n",
    "<br />\n",
    "$\\Large \\widehat{\\nabla}_\\theta J(\\theta)=\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(A_t^{(i)}\\mid S_t^{(i)})\\sum_{h=t}^{T-1}\\gamma^h R_{h+1}^{(i)}$\n",
    "\n",
    "* You can then add a **baseline** to your gradient estimator:<newline/>\n",
    "    $\\Large \\widehat{\\nabla}_\\theta J(\\theta)=\\frac{1}{N}\\sum_{i=1}^N\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(A_t^{(i)}\\mid S_t^{(i)})\\sum_{h=t}^{T-1}\\left(\\gamma^h R_{h+1}^{(i)} - b_h\\right)$ \n",
    "<br />\n",
    "Any baseline that does not depend on actions keeps the gradient estimator unbiased, and a good baseline can reduce variance. A common choice is the average-reward baseline \n",
    "<br />\n",
    "$\\Large b_h = \\frac{1}{N}\\sum_{i=1}^N\\sum_{h=t}^{T-1}\\gamma^h R_{h+1}^{(i)}$\n",
    "<br />\n",
    "See <a href=\"https://www.sciencedirect.com/science/article/pii/S0893608008000701\">this paper</a> for more sophisticated variance-reduction baselines\n",
    "\n",
    "\n",
    "* **Actor critic (advanced)**: instead of estimating the Q functions with the return-to-go, you can learn it using your favourite policy evaluation algorithm\n",
    "<br />\n",
    "$\\Large \\widehat{\\nabla}_\\theta J(\\theta)=\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(A_t\\mid S_t)\\hat{Q}_\\theta(S_t, A_t)$\n",
    "<br />\n",
    "Designing actor-critic algorithms is a subtle art. A general principle is that the critic (the approximated Q function) should be updated at a faster rate than the actor (the policy), e.g. by using a larger learning rate.\n",
    "<a href=\"https://papers.nips.cc/paper/1999/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html\">Here</a> is a good reference \n",
    "\n",
    "\n",
    "### Policy class\n",
    "* **State features**: The policy class we have considered so far may be too simple to achieve optimal performance. You may try to handcraft some state features $\\phi:\\mathcal{S}\\to\\mathbb{R}^d$ and use the following policy class\n",
    "<br />\n",
    "$\\Large \\pi_\\theta(a\\mid s) = \\frac{\\exp(\\theta_a\\cdot \\phi(s))}{\\sum_{a'\\in\\mathcal{A}}\\exp(\\theta_{a'}\\cdot \\phi(s))}$\n",
    "\n",
    "* **Go deep! (advanced)** If you can't think of good features, you can try to implement a deep policy class where the exponential weights are given by a neural network that takes states as inputs, and the policy parameters are the weights of the network. Here is a possible partial implementation using `stax` (https://jax.readthedocs.io/en/latest/jax.example_libraries.stax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55505cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.example_libraries import stax\n",
    "\n",
    "class DeepAgent(PGSoftmaxAgent):\n",
    "    def __init__(self, state_dim, n_actions, hidden=8):\n",
    "        #neural network with one hidden layer\n",
    "        self.nn_init, self.model = stax.serial(\n",
    "            stax.Dense(hidden), #state as input\n",
    "            stax.Tanh, #tanh activations\n",
    "            stax.Dense(n_actions)) #one output per action (the unnormalized log-probabilities)        )\n",
    "        super().__init__(state_dim, n_actions)\n",
    "        \n",
    "    def reset(self, seed=0):\n",
    "        key = random.PRNGKey(seed)\n",
    "        output_shape, params_init = self.nn_init(key, (-1, self.state_dim)) #initialization of network weights\n",
    "        self.param = params_init #policy parameters are network weights\n",
    "    \n",
    "    def log_policy(self, policy_parameters, state):\n",
    "        x = jnp.array(state)\n",
    "        unn_log_probs = self.model(policy_parameters, x) #the network gives the unnormalized log probabilities\n",
    "        #..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ba73c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
